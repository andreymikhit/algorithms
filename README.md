# algorithms
GeekBrains
# Алгоритмы анализа данных

### Урок 1. Алгоритм линейной регрессии. Градиентный спуск
Реализовать оптимизацию методом градиентного спуска для x^2(np.sin(0,5_x)^2+1). Какие параметры шага и количества итераций оптимальны? При каких значениях шага оптимизиция не выходит из локальных минимумов? Подробные условия смотрите в .ipynb файле, приложенном к материалам.

### Урок 2. Масштабирование признаков. L1- и L2-регуляризация. Стохастический градиентный спуск
Задание дано в .ipynb файле в материалах к данному уроку. В ходе задания вам потребуется выполнить модификации выражений для рассчёта функции потерь линейной регресси и её градиента, связанные с добавлением регулярязационной l2 поправки.

### Урок 3. Логистическая регрессия. Log Loss
Определите AUC_ROC и AUC_PR для модели логистической регрессии на тестовой выборке. Используйте реализацию модели в виде класса и функцию confusion, чтобы рассчитать TPR, FPR, precision, recal для различных порогов вероятности.
Дополнительное задание(не обязательно): Получите такие же характеристики для линейной регрессии и сравните две модели.
Подробности в .ipynb файле

### Урок 4. Алгоритм построения дерева решений
1) реализовать аналогичного изученному дерева решений, использующего энтропию Шенонна в качестве критерия информативности и сравнить точность(accuracy, balanced_accuracy) достигаемую на используемых синтетических данных с точностью разобранной на уроке реализации
2) реализовать ограничение глубины дерева при построении. Построить на этом же датасете дерево, у которого максимальная глубна листа равна 2.

### Урок 5. Случайный лес
Реализовать оценку Out-of-Bag ошибок для каждого из деревьев леса

### Урок 6. Градиентный бустинг (AdaBoost)
Домашнее задание: Реализовать адаптивный бустинг использующий Логистическую Регрессию и меру ошибок LogLoss. Сравнить с точностью адаптивного бустинга на деревьях решений. Для сбора предсказаний можно использовать ту же функцию predict что и для бустинга на деревьях
Подробности в файле "Адаптивный бустинг.ipynb"

### Урок 7. Классификация с помощью KNN. Кластеризация K-means
Модифицировать реализацию KNN из методички так, чтобы больший вес получали наиболее близко расположенные объекты
Реализовать иерархическую кластеризацию с любой из межкластерных метрик, кроме рассмотренной на вебинаре(Average Linkage Distance). Межкластерные метрики(Intercuster Distance) находятся здесь: https://www.geeksforgeeks.org/ml-intercluster-and-intracluster-distance/

### Урок 8. Снижение размерности данных
Для любой модели из курса(кроме KNN) и данных на которых она строилась: примнить метод главных компонент к данным, и сравнить работу модели на исходных данных с работой на данных где в качестве признаков используются две главные компоненты.
